{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project\n",
    "\n",
    "This notebook together with the 'final_project' directory in this repository contains all the data and code used to create a final project on the course `Practical analysis of noisy and uneven time series` on `Advanced Data Analytics` master program on the University of Belgrade.\n",
    "In the notebook, we will work with light curves with the main focus of applying prestatistical analyses using packages such as `numpy` and `pandas` and modelling neural processes with the implementation of `QNPy` a proprietary python package for modelling quasar time series using neural processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Damir Bogdan <damirbogdan39@gmail.com>'\n",
    "__version__ = '20230411'\n",
    "__keywords__ = ['lightcurve', 'quasars', 'clustering', 'nerual processes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Utilities\n",
    "from utils import count_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNPy dependencies\n",
    "\n",
    "import QNPy #Importing the package\n",
    "\n",
    "# Preprocessing\n",
    "from QNPy import Preprocess as pr #Importing Preprocess module from the package\n",
    "from QNPy.Preprocess import transform #importing the funcion transform for transformation the data\n",
    "from QNPy.Preprocess import * #importing all external packages from Preprocess\n",
    "\n",
    "# Splitting and training\n",
    "from QNPy import SPLITTING_AND_TRAINING as st\n",
    "from QNPy.SPLITTING_AND_TRAINING import *\n",
    "\n",
    "# Prediction \n",
    "from QNPy import PREDICTION as pred #Importing PREDICTION module from the package\n",
    "from QNPy.PREDICTION import * #Importing all packages from PREDICTION module\n",
    "from QNPy.PREDICTION import plot_function #The functions plot_function must be imported separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data path\n",
    "\n",
    "data_path = 'kriveu/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will import all individual light curves as a dataframe. This will allow us to inspect every individual light curve using common padnas functions but also visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the light curves as dataframes\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(data_path)\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "# Read each file into a DataFrame and store it in the list\n",
    "for file_name in all_files:\n",
    "    try:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        \n",
    "        # Read the DataFrame and create a variable with the file name\n",
    "        df = pd.read_csv(file_path)\n",
    "        globals()['lc_' + os.path.splitext(file_name)[0]] = df\n",
    "        \n",
    "        # Add the DataFrame to the list\n",
    "        all_dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a combined dataframe called `combined_df` so we can inspect them all as one time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataframe\n",
    "\n",
    "# Create an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file and read it into a DataFrame\n",
    "for file in all_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    \n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(file_path)  # Adjust the read function based on your file format\n",
    "    \n",
    "    # Add a new column with the file name\n",
    "    df['light_curve'] = file\n",
    "    \n",
    "    # Concatenate the current DataFrame with the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to print the shape and name of each DataFrame\n",
    "for file_name, df in zip(all_files, all_dataframes):\n",
    "    df_name = 'lc_' + os.path.splitext(file_name)[0]\n",
    "    print(f\"{df_name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataframes are of different shapes.\n",
    "Their first dimension differs which means they are not equal in the number of rows which represent the number of timestamps. The smallest light curve consists out of 107 records, and the largest one consist out of 151 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing lightcurves\n",
    "\n",
    "for i, df in enumerate(all_dataframes):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    trace = go.Scatter(x=df['mjd'], y=df['mag'], mode='markers', name='mag', marker=dict(size=4))\n",
    "\n",
    "    error_bars = go.Scatter(\n",
    "        x=df['mjd'],\n",
    "        y=df['mag'],\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=df['magerr'],\n",
    "            visible=True\n",
    "        ),\n",
    "        mode='markers',\n",
    "        marker=dict(size=4),\n",
    "        name='mag with error bars'\n",
    "    )\n",
    "\n",
    "    fig.add_trace(trace)\n",
    "    fig.add_trace(error_bars)\n",
    "\n",
    "    fig.update_xaxes(title_text='MJD (Modified Julian Date)')\n",
    "    fig.update_yaxes(title_text='Magnitude')\n",
    "\n",
    "    fig.update_layout(title_text=f\"Time Series with Error Bars - Plot {i + 1}\", showlegend=True)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing box and whisker plots and histograms for all individual light curves\n",
    "\n",
    "for i, df in enumerate(all_dataframes):\n",
    "    fig = px.histogram(df, x='mag', marginal='box', nbins=30, title=f\"KDE Plot for 'mag' - Plot {i + 1}\")\n",
    "\n",
    "    fig.update_xaxes(title_text='Magnitude')\n",
    "    fig.update_yaxes(title_text='Density')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics of each light curve\n",
    "\n",
    "combined_df.groupby('light_curve')[['mag','magerr']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ge the number of outliers in each light curve\n",
    "\n",
    "combined_df.groupby('light_curve')[['mag','magerr']].apply(count_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All light curves together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all light curves on one graph\n",
    "\n",
    "fig = px.scatter()\n",
    "\n",
    "colors = px.colors.qualitative.Set1 + px.colors.qualitative.Pastel1 # Combine two color palletes because Set1 isn't enough\n",
    "\n",
    "for i, df in enumerate(all_dataframes):\n",
    "    trace = go.Scatter(\n",
    "        x=df['mjd'],\n",
    "        y=df['mag'],\n",
    "        mode='markers',\n",
    "        name=f'Plot {i + 1}',\n",
    "        marker=dict(size=4, color=colors[i % len(colors)])\n",
    "    )\n",
    "\n",
    "    error_bars = go.Scatter(\n",
    "        x=df['mjd'],\n",
    "        y=df['mag'],\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=df['magerr'],\n",
    "            visible=True\n",
    "        ),\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color=colors[i % len(colors)]),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.add_trace(trace)\n",
    "    fig.add_trace(error_bars)\n",
    "\n",
    "fig.update_xaxes(title_text='MJD (Modified Julian Date)')\n",
    "fig.update_yaxes(title_text='Magnitude')\n",
    "\n",
    "fig.update_layout(title_text=\"Time Series with Error Bars - All Plots\", showlegend=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNPy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./light_curves\"\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "df_list = (pd.read_csv(file) for file in files)\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the light curves\n",
    "\n",
    "padding = pr.backward_pad_curves('./light_curves', './padded_lc', desired_observations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to padded data \n",
    "PADDED_PATH = \"./padded_lc\"\n",
    "\n",
    "# Path to save preprocesses data\n",
    "PREPROC_PATH = \"./preproc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_files = os.listdir(PADDED_PATH)\n",
    "padded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points, trcoeff = pr.transform_and_save(padded_files, PADDED_PATH, PREPROC_PATH, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_files = os.listdir(PREPROC_PATH) #listing the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paths to train, val and test directories\n",
    "\n",
    "TRAIN_DIR = \"./dataset/train/\"\n",
    "VAL_DIR = \"./dataset/val/\"\n",
    "TEST_DIR = \"./dataset/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "# st.split_data(preproc_files, PREPROC_PATH, TRAIN_DIR, TEST_DIR, VAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paths to the directories\n",
    "\n",
    "TRAIN_DIR_PATH = \"./dataset/train\"\n",
    "VAL_DIR_PATH = \"./dataset/val\"\n",
    "TEST_DIR_PATH = \"./dataset/test\"\n",
    "\n",
    "# Defining the path to the model\n",
    "MODEL_PATH = \"./output/cnp_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining batch size \n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and validation loaders\n",
    "\n",
    "train_loader, val_loader = st.get_data_loaders(TRAIN_DIR_PATH, VAL_DIR_PATH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model and optimizer\n",
    "\n",
    "model, optimizer, criterion, mse_metric, mae_metric = st.create_model_and_optimizer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "history_loss_train, history_loss_val, \\\n",
    "history_mse_train, history_mse_val, \\\n",
    "history_mae_train, history_mae_val, \\\n",
    "epoch_counter_train_loss, epoch_counter_train_mse, epoch_counter_train_mae, \\\n",
    "epoch_counter_val_loss, epoch_counter_val_mse, epoch_counter_val_mae = \\\n",
    "st.train_model(model,\n",
    "               train_loader, val_loader,\n",
    "               criterion, optimizer,\n",
    "               1, 6000, 3000,\n",
    "               mse_metric, mae_metric,\n",
    "               device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file names\n",
    "file_names = [\"history_loss_train.csv\", \"history_loss_val.csv\",\n",
    "              \"history_mse_train.csv\", \"history_mse_val.csv\",\n",
    "              \"history_mae_train.csv\", \"history_mae_val.csv\",\n",
    "              \"epoch_counter_train_loss.csv\", \"epoch_counter_train_mse.csv\", \"epoch_counter_train_mae.csv\",\n",
    "              \"epoch_counter_val_loss.csv\", \"epoch_counter_val_mse.csv\", \"epoch_counter_val_mae.csv\"]\n",
    "\n",
    "# Define the lists\n",
    "lists = [history_loss_train, history_loss_val,\n",
    "         history_mse_train, history_mse_val,\n",
    "         history_mae_train, history_mae_val,\n",
    "         epoch_counter_train_loss, epoch_counter_train_mse, epoch_counter_train_mae,\n",
    "         epoch_counter_val_loss, epoch_counter_val_mse, epoch_counter_val_mae]\n",
    "\n",
    "#Running the function for saving all lists with histories\n",
    "save_list = st.save_lists_to_csv(file_names, lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD COMMENT FOR COUNTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your history_loss_train CSV file\n",
    "history_loss_train_file = './history_loss_train.csv'  \n",
    "# Replace with the path to your history_loss_val CSV file\n",
    "history_loss_val_file = './history_loss_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_loss CSV file\n",
    "epoch_counter_train_loss_file = './epoch_counter_train_loss.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"./\"\n",
    "\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "def create_training_df(csv_paths):\n",
    "    \n",
    "    dataframes=[]\n",
    "\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path, header=None, sep=',').T\n",
    "        df.rename({0:path[:-4]}, axis=1, inplace=True)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    training_df = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return training_df\n",
    "\n",
    "\n",
    "training_df = create_training_df(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'epoch_counter_train_loss', 'history_loss_train', and 'history_loss_val' are columns in training_df\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=training_df['epoch_counter_train_loss'], y=training_df['history_loss_train'], mode='lines', name='Train LOSS'))\n",
    "fig.add_trace(go.Scatter(x=training_df['epoch_counter_train_loss'], y=training_df['history_loss_val'], mode='lines', name='Validation LOSS'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"LogProbLOSS\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"LOSS\",\n",
    "    legend=dict(x=0, y=1, traceorder='normal', orientation='h'),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Logprobloss after training\n",
    "logprobloss = st.plot_loss(history_loss_train_file, history_loss_val_file, epoch_counter_train_loss_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lineplot(x='epoch_counter_train_loss', y='history_loss_train', data=training_df, label='Train LOSS')\n",
    "sns.lineplot(x='epoch_counter_train_loss', y='history_loss_val', data=training_df, label='Validation LOSS')\n",
    "plt.title(\"LosProbLOSS\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace with the path to your history_mse_train CSV file\n",
    "history_mse_train_file = './history_mse_train.csv'\n",
    "# Replace with the path to your history_mse_val CSV file\n",
    "history_mse_val_file = './history_mse_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_mse CSV file\n",
    "epoch_counter_train_mse_file = './epoch_counter_train_mse.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the MSE metric after training\n",
    "msemetric=st.plot_mse(history_mse_train_file, history_mse_val_file, epoch_counter_train_mse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your history_mae_train CSV file\n",
    "history_mae_train_file = './history_mae_train.csv'\n",
    "# Replace with the path to your history_mae_val CSV file\n",
    "history_mae_val_file = './history_mae_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_mae CSV file\n",
    "epoch_counter_train_mae_file = './epoch_counter_train_mae.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the MAE metric after training\n",
    "maemetric=st.plot_mae(history_mae_train_file, history_mae_val_file, epoch_counter_train_mae_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model\n",
    "\n",
    "save = st.save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './output/predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function call is optional, it is meant to be executed when output and it's child directiores are not empty.\n",
    "\n",
    "pred.prepare_output_dir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model load_trained_model function\n",
    "\n",
    "model = pred.load_trained_model(MODEL_PATH, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD THAT YOU HAD TO TROUBLESHOOT WHY PR WOULD NOT WORK. IT'S BECASE THE PREPROCESSING IS CALLED PR AND ALSO PREDICTION IS CALLED PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion, mse_metric = pred.get_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"./dataset/test\"\n",
    "\n",
    "    pred.remove_padded_values_and_filter(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"./dataset/val\"\n",
    "\n",
    "    pred.remove_padded_values_and_filter(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"./dataset/val\"\n",
    "\n",
    "    pred.remove_padded_values_and_filter(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = pred.load_test_data(TEST_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = pred.load_train_data(TRAIN_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = pred.load_val_data(VAL_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pred.plot_light_curves_from_test_set(model, test_loader, criterion, mse_metric, plot_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_metric = pred.save_test_metrics('./output/predictions/', test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction and plotting the train data\n",
    "train_metrics=pred.plot_light_curves_from_train_set(train_loader, model, criterion, mse_metric, plot_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_metric=pred.save_train_metrics('./output/predictions/', train_metrics)#saving the train metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics=pred.plot_light_curves_from_val_set(model, val_loader, criterion, mse_metric, plot_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_val_metrics=pred.save_val_metrics('./output/predictions/', val_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
