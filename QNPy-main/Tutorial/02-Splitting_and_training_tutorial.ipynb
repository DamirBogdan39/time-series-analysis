{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4a32f9",
   "metadata": {},
   "source": [
    "# Splitting and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568476de",
   "metadata": {},
   "source": [
    "Before running this script, you must create the following folders in the directory where your Python notebook is located:\n",
    "1. ./dataset/train -- folder for saving data for traing after splitting your original dataset\n",
    "2. ./dataset/test -- folder for test data \n",
    "3. ./dataset/val -- folder for validation data\n",
    "4. ./output -- folder where you are going to save your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83322b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QNPy #Importing the package\n",
    "from QNPy import SPLITTING_AND_TRAINING as st #Importing SPLITTING_AND_TRAINING module from the package\n",
    "from QNPy.SPLITTING_AND_TRAINING import * #Importing all packages from SPLITTING_AND_TRAINING module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc36e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SRC = \"./preproc\" #Path to transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fcc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(DATA_SRC) #listing the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eaa67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to TRAIN, TEST and VAL folders where your splitted data will be saved. \n",
    "#You can directly enter this informations in split_data function\n",
    "TRAIN_FOLDER = './dataset/train/'\n",
    "TEST_FOLDER = './dataset/test/'\n",
    "VAL_FOLDER = './dataset/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce436a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 319.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#running the function for splitting the data\n",
    "st.split_data(files, DATA_SRC, TRAIN_FOLDER, TEST_FOLDER, VAL_FOLDER) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc877ae4",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce784a5",
   "metadata": {},
   "source": [
    "# Special note for mac os users: \n",
    "\n",
    "When creating folders with mac operating systems, hidden .DS_Store files may be created. The user must delete these files before starting training from each folder. The best way is to go into each folder individually and run the command:\n",
    "\n",
    "!rm -f .DS_Store\n",
    "\n",
    "Important note: Deleting files using the \"delete\" directly in the folders does not remove hidden files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097d1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TRAIN = \"./dataset/train\" #path to train folder\n",
    "DATA_PATH_VAL = \"./dataset/val\" #path to val folder\n",
    "\n",
    "MODEL_PATH = \"./output/cnp_model.pth\" #path for saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9780754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32 #Defining the batch size, it should remain 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7e3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the function for getting the data loaders of data that are going to be trained\n",
    "trainLoader, valLoader = st.get_data_loaders(DATA_PATH_TRAIN, DATA_PATH_VAL, BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d3a931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the device for testing, it can be CPU of CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad2e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runing the function fo creating the model (deterministic model is default) \n",
    "#and optimiser (LogProbLoss), mseMetric (MSELoss)\n",
    "model, optimizer, criterion, mseMetric, maeMetric = st.create_model_and_optimizer(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d45da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 175/3000 [00:15<04:06, 11.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#function for training the data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# numbers that we give in this example are: \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#1 - number of training runs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#3000 - number of epochs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#2000 - number of early stopping limit\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# These numbers are optional and can be changed as needed.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history_loss_train, history_loss_val, \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m history_mse_train, history_mse_val, \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m history_mae_train, history_mae_val, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m epoch_counter_train_loss, epoch_counter_train_mse, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m epoch_counter_train_mae, epoch_counter_val_loss, \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m epoch_counter_val_mse, epoch_counter_val_mae \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damir/Projects/time-series-analysis/QNPy-main/Tutorial/02-Splitting_and_training_tutorial.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model, trainLoader, valLoader, criterion, optimizer, \u001b[39m1\u001b[39;49m, \u001b[39m3000\u001b[39;49m, \u001b[39m2000\u001b[39;49m, mseMetric, maeMetric, device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/QNPy/SPLITTING_AND_TRAINING.py:172\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, trainLoader, valLoader, criterion, optimizer, num_runs, EPOCHS, EARLY_STOPPING_LIMIT, mseMetric, maeMetric, device)\u001b[0m\n\u001b[1;32m    170\u001b[0m total_mse_val \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    171\u001b[0m total_mae_val \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 172\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m valLoader:\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Unpack data\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     context_x, context_y, target_x, target_y, target_test_x, measurement_error \u001b[39m=\u001b[39m data[\n\u001b[1;32m    175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontext_x\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     ], data[\u001b[39m\"\u001b[39m\u001b[39mcontext_y\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mtarget_x\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mtarget_y\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mtarget_test_x\u001b[39m\u001b[39m\"\u001b[39m], data[\n\u001b[1;32m    177\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmeasurement_error\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     ]\n\u001b[1;32m    180\u001b[0m     \u001b[39m# Move to GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/QNPy/CNP_DATASETCLASS.py:29\u001b[0m, in \u001b[0;36mLighCurvesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Read data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m lcName \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_paths[idx])\n\u001b[0;32m---> 29\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_paths[idx])\n\u001b[1;32m     30\u001b[0m x_data \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m y_data \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mcont\u001b[39m\u001b[39m'\u001b[39m] \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1234\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1233\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:153\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_parse_dates_presence(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_noconvert_columns()\n\u001b[1;32m    155\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:208\u001b[0m, in \u001b[0;36mCParserWrapper._set_noconvert_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m col_indices \u001b[39m=\u001b[39m [names_dict[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames]  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m noconvert_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_noconvert_dtype_columns(\n\u001b[1;32m    209\u001b[0m     col_indices,\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnames,  \u001b[39m# type: ignore[has-type]\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m noconvert_columns:\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mset_noconvert(col)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py:610\u001b[0m, in \u001b[0;36mParserBase._set_noconvert_dtype_columns\u001b[0;34m(self, col_indices, names)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFilled \u001b[39m\u001b[39m{\u001b[39;00mna_count\u001b[39m}\u001b[39;00m\u001b[39m NA values in column \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m!s}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> 610\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_noconvert_dtype_columns\u001b[39m(\n\u001b[1;32m    612\u001b[0m     \u001b[39mself\u001b[39m, col_indices: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], names: Sequence[Hashable]\n\u001b[1;32m    613\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mset\u001b[39m[\u001b[39mint\u001b[39m]:\n\u001b[1;32m    614\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39m    Set the columns that should not undergo dtype conversions.\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39m    A set of integers containing the positions of the columns not to convert.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     usecols: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#function for training the data\n",
    "# numbers that we give in this example are: \n",
    "#1 - number of training runs\n",
    "#3000 - number of epochs\n",
    "#2000 - number of early stopping limit\n",
    "# These numbers are optional and can be changed as needed.\n",
    "history_loss_train, history_loss_val, \\\n",
    "history_mse_train, history_mse_val, \\\n",
    "history_mae_train, history_mae_val, \\\n",
    "epoch_counter_train_loss, epoch_counter_train_mse, \\\n",
    "epoch_counter_train_mae, epoch_counter_val_loss, \\\n",
    "epoch_counter_val_mse, epoch_counter_val_mae = st.train_model(\n",
    "    model, trainLoader, valLoader, criterion, optimizer, 1, 3000, 2000, mseMetric, maeMetric, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2407ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file names for saving the lists for all histories\n",
    "file_names = [\"history_loss_train.csv\", \"history_loss_val.csv\", \"history_mse_train.csv\", \"history_mse_val.csv\",\n",
    "              \"history_mae_train.csv\", \"history_mae_val.csv\", \"epoch_counter_train_loss.csv\",\n",
    "              \"epoch_counter_train_mse.csv\", \"epoch_counter_train_mae.csv\", \"epoch_counter_val_loss.csv\",\n",
    "              \"epoch_counter_val_mse.csv\", \"epoch_counter_val_mae.csv\"]\n",
    "\n",
    "# Define the lists\n",
    "lists = [history_loss_train, history_loss_val, history_mse_train, history_mse_val, history_mae_train,\n",
    "         history_mae_val, epoch_counter_train_loss, epoch_counter_train_mse, epoch_counter_train_mae,\n",
    "         epoch_counter_val_loss, epoch_counter_val_mse, epoch_counter_val_mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the function for saving all lists with histories\n",
    "save_list= st.save_lists_to_csv(file_names, lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da500045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your history_loss_train CSV file\n",
    "history_loss_train_file = './history_loss_train.csv'  \n",
    "# Replace with the path to your history_loss_val CSV file\n",
    "history_loss_val_file = './history_loss_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_loss CSV file\n",
    "epoch_counter_train_loss_file = './epoch_counter_train_loss.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Logprobloss after training\n",
    "logprobloss=st.plot_loss(history_loss_train_file, history_loss_val_file, epoch_counter_train_loss_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace with the path to your history_mse_train CSV file\n",
    "history_mse_train_file = './history_mse_train.csv'\n",
    "# Replace with the path to your history_mse_val CSV file\n",
    "history_mse_val_file = './history_mse_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_mse CSV file\n",
    "epoch_counter_train_mse_file = './epoch_counter_train_mse.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the MSE metric after training\n",
    "msemetric=st.plot_mse(history_mse_train_file, history_mse_val_file, epoch_counter_train_mse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed16428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your history_mae_train CSV file\n",
    "history_mae_train_file = './history_mae_train.csv'\n",
    "# Replace with the path to your history_mae_val CSV file\n",
    "history_mae_val_file = './history_mae_val.csv'  \n",
    "# Replace with the path to your epoch_counter_train_mae CSV file\n",
    "epoch_counter_train_mae_file = './epoch_counter_train_mae.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the MAE metric after training\n",
    "maemetric=st.plot_mae(history_mae_train_file, history_mae_val_file, epoch_counter_train_mae_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save=st.save_model(model, MODEL_PATH)#saving the trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
